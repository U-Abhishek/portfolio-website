{
  "projects": [


    {
      "name": "StyleVec API",
      "description": "A powerful API for clothing search and retrieval using vector embeddings and computer vision. Enables users to search for clothing items using images, create embeddings, and perform segmentation for detailed clothing detection. Features include vector similarity search, clothing segmentation with masks and bounding boxes, and MCP server integration for AI assistants.",
      "date": "2025-05",
      "ieee": "",
      "live": "",
      "github": "https://github.com/U-Abhishek/StyleVec_API",
      "report": "",
      "image": "/images/projects/stylevec_api.png",
      "youtube": "",
      "tags": ["AI Systems"],
      "summary": "Clothing search API with vector embeddings, similarity search, segmentation, and MCP integration for AI assistants.",
      "featured": true
    },

    {
      "name": "SavorSync",
      "description": "A culturally immersive cooking platform built with Langchain, OpenAI, DSPy, MongoDB, Express, and Node.js. Developed sequential AI workflows in Langchain to generate cultural recipes and stories with consistency. Implemented evaluation-driven prompt optimization for RAG system using DSPy and Promptim. Conducted structured testing sessions with 44 users to drive targeted UI/UX improvements.",
      "date": "2025-03",
      "ieee": "",
      "live": "https://savorsync-myproject.netlify.app/",
      "github": "https://github.com/U-Abhishek/SavorSync-API",
      "report": "",
      "image": "/images/projects/savorsync.png",
      "youtube": "",
      "tags": ["AI Systems"],
      "summary": "Cultural cooking platform with Langchain AI workflows, DSPy optimization, and user-driven UI/UX improvements.",
      "featured": true
    },
    
    {
      "name": "Content-Based Image Retrieval",
      "description": "This project explores the concept of matching images by specific features such as color, texture, image section, or using deep networks. It also introduces different distance metrics to compare 2 images of different colors, textures, etc, such as histogram intersection. When looking at matching images by features it's important to understand which features are of interest to match to other images, thus combining and weighing different or similar features can be helpful to achieve the desired result.",
      "date": "2024-01",
      "ieee": "",
      "live": "",
      "github": "",
      "report": "https://drive.google.com/file/d/1bxmgPQXGWOioHoGkAmJCU9crCmD2XjIh/view",
      "image": "/images/projects/Content-Based Image Retrieval.png",
      "youtube": "",
      "tags": ["Machine Learning / Vision"],
      "summary": "Feature-based image matching using color, texture, regions, and deep features; compares images with metrics like histogram intersection.",
      "featured": true
    },
    {
      "name": "Real-time 2D Object Recognition",
      "description": "This project introduces methods to compare image frames within a video sequence for object recognition. We use region-based object recognition methods such as moments, percent fill, etc to compare objects. The program takes a training database of at least 5 objects of multiple points to then compare to new objects (known or unknown), if the objects are known the system will classify the object. With comparison, multiple distance metrics can be used to compare objects, such as L2 distance and KNN classification, which we will use in this project.",
      "date": "2024-02",
      "ieee": "",
      "live": "",
      "github": "https://github.com/U-Abhishek/Real-time-2D-object-detection",
      "report": "https://drive.google.com/file/d/1CPJmNOlbd0gZSQDC1_rxCFZbUmbiTfSl/view",
      "image": "/images/projects/realtime object detection.png",
      "youtube": "",
      "tags": ["Machine Learning / Vision"],
      "summary": "Region-based object recognition in video using moments, percent fill, and KNN/L2 metrics with a small training database.",
      "featured": false
    },
    {
      "name": "Calorie-Counter",
      "description": "This project provides an interface to track food and calories per day with a scale supplied by the user. Foods included in our database include: Cheesecake, tacos, scallops, pizza, pho, donuts, pancake, spaghetti, strawberry, and bananas.",
      "date": "2024-03",
      "ieee": "",
      "live": "",
      "github": "https://github.com/U-Abhishek/Calorie-Counter",
      "report": "",
      "image": "/images/projects/calorie counter.png",
      "youtube": "",
      "tags": ["Machine Learning / Vision"],
      "summary": "Simple interface to track daily foods and calories with a user-supplied scale.",
      "featured": false
    },
    {
      "name": "Road Surface Segmentation",
      "description": "Road surface segmentation is a crucial aspect of self-driving cars, autonomous robots, and advanced driving assistant systems, which attracted much research attention to this topic. Deep learning models are being developed with the availability of large road segmentation datasets. However, the diversity of data obtainable is finite and it is not transparent to what degree techniques generalize beyond the explicit datasets they were trained on. In this paper, we propose a novel training process to improve the cross-dataset performance of road segmentation systems by forcing the model to learn features that are highly relevant to the output required.",
      "date": "2022-02",
      "ieee": "",
      "live": "",
      "github": "https://github.com/U-Abhishek/Road-Segmentation",
      "report": "https://drive.google.com/file/d/1RhgZgy-aXfWRKdqMsXR65-mGC_Br-2no/view",
      "image": "/images/projects/road surface segmentation.png",
      "youtube": "",
      "tags": ["Machine Learning / Vision"],
      "summary": "Training strategy to improve cross-dataset generalization for road surface segmentation using deep learning.",
      "featured": false
    },
    {
      "name": "Customized Region Proposal based Traffic Sign Detection",
      "description": "In the present work, traffic signs were recognized by a deep neural network architecture from a camera image. The customized object proposal method is proposed to localize the region of interest. The number of regions generated by the proposed method is five times fewer than selective search segmentation, this makes the process faster than selective search segmentation as the time spent on false regions proposed by selective search is saved. The proposed method is computationally efficient and simple as compared to modern faster regional convolutional neural networks, where Convolutional layers (RPN) are embedded in the network to determine the region of interest. The proposed system can be utilized for developing a smart driver-assistant system and self-driving cars.",
      "date": "2021-07",
      "ieee": "https://ieeexplore.ieee.org/document/9850716",
      "live": "",
      "github": "https://github.com/U-Abhishek/Region-Proposal-For-Traffic-Sign-Detection",
      "report": "",
      "image": "/images/projects/Customized Region Proposal based Traffic Sign Detection.png",
      "youtube": "",
      "tags": ["Machine Learning / Vision"],
      "summary": "Lightweight traffic sign detection using a custom region proposal method that reduces proposals vs. selective search.",
      "featured": false
    },
    {
      "name": "Leo Explore",
      "description": "The Reconnaissance Bot is an advanced autonomous navigation system designed to explore and map unknown environments. Utilizing a sophisticated navigation stack based on frontier exploration, it integrates Simultaneous Localization and Mapping (SLAM) to gather essential spatial information. The system consists of three key modules: the Frontier Server, which uses K-means clustering to identify unexplored regions; the Global Planner, which optimizes paths using an adapted A-star algorithm; and the Local Planner, employing a Model Predictive Path Integral (MPPI) method for obstacle-aware motion. Motivated by the need for autonomous systems to safely navigate hazardous or inaccessible areas, the Reconnaissance Bot has been successfully tested on the TurtleBot3 Burger platform, showcasing its potential for real-world applications such as disaster response and military reconnaissance.",
      "date": "2024-04",
      "ieee": "",
      "live": "",
      "github": "https://github.com/U-Abhishek/Leo_explore",
      "report": "https://drive.google.com/file/d/1-959kS4BxK7TbUYK5-S4CK7VB4_yWfh2/view",
      "image": "/images/projects/reconnaissance bot.png",
      "youtube": "https://www.youtube.com/watch?v=fyIsYrZ6wks&list=TLGG8CfmGmJbyOgyNDA5MjAyNQ&t=50s",
      "tags": ["Robotics"],
      "summary": "Autonomous frontier-exploration stack (SLAM, K-means frontiering, adapted A*, MPPI) validated on TurtleBot3.",
      "featured": true
    },
    {
      "name": "Mapping Boston Streets using an Autonomous Car",
      "description": "This project aims to create a detailed point cloud map of selected streets in Boston using the LEGO-LOAM SLAM (Simultaneous Localization and Mapping) algorithm on the NUANCE autonomous vehicle. To accommodate the specific conditions and data collected during our mapping runs, the standard LEGO-LOAM system was modified and optimized for our dataset. LEGO-LOAM is a lightweight and computationally efficient LiDAR-based odometry and mapping system designed for ground-based autonomous vehicles in ROS environments. In this implementation, the system processes point cloud data from a Velodyne VLP-16 LiDAR sensor and produces real-time 6-degree-of-freedom pose estimation. For this project, data was collected on various streets in Boston using the NUANCE vehicle equipped with multiple sensors and recorded in Rosbag format.",
      "date": "2023-12",
      "ieee": "",
      "live": "",
      "github": "https://github.com/adityaaspat/Robotics/tree/main/LeGO-LOAM/LeGO-LOAM",
      "report": "",
      "image": "/images/projects/mapping streets of boston.png",
      "youtube": "",
      "tags": ["Robotics"],
      "summary": "Urban LiDAR mapping with LEGO-LOAM on NUANCE vehicle; optimized pipeline for our dataset.",
      "featured": false
    },
    {
      "name": "Optimizing Autonomous Vehicle Positioning Using IMU and GPS",
      "description": "This project focuses on analyzing data collected from the NUANCE autonomous vehicle, equipped with an Inertial Measurement Unit (IMU) and GPS sensors, during stationary and moving states on Northeastern University's campus. The primary goal is to assess vehicle position, orientation, and forward velocity using sensor fusion techniques.",
      "date": "2023-11",
      "ieee": "",
      "live": "",
      "github": "",
      "report": "https://drive.google.com/file/d/1uXPaIb9RzE0dKg7XVhpmjyoVX1k5qCPO/view",
      "image": "/images/projects/sensorfusion.png",
      "youtube": "",
      "tags": ["Robotics"],
      "summary": "Sensor fusion analysis of IMU and GPS to estimate pose and velocity on campus runs.",
      "featured": false
    },
    {
      "name": "Intelligent MPPT Implementation ",
      "description": "A comparative analysis of traditional Perturb & Observe and ANN-based MPPT for solar PV systems, showcasing improved tracking efficiency and stability under varying conditions with intelligent MPPT.",
      "date": "2022-03",
      "ieee": "https://ieeexplore.ieee.org/document/9909115",
      "live": "",
      "github": "",
      "report": "",
      "image": "/images/projects/mppt.png",
      "youtube": "",
      "tags": ["Machine Learning / Vision"],
      "summary": "Comparison of P&O and ANN-based MPPT on a boost converter for improved tracking and stability.",
      "featured": false
    }
  ]
}
